# -*- coding: utf-8 -*-
"""Emplay_t.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1fjSDXONM2emBFyApM-m5F59JKVv_mLxp

##### Libraries
"""

!pip install requests -q
!pip install html5lib -q
!pip install bs4 -q
!pip install selenium -q
!pip install webdriver_manager -q
!pip install beautifulsoup4 -q
!pip install -q openai
!pip install rouge_score -q
!pip install transformers -q
!pip install evaluate -q

import requests
from bs4 import BeautifulSoup
import pandas as pd
import numpy as np
import os
import openai
import time
import matplotlib.pyplot as plt
import evaluate
from transformers import pipeline
import torch
import re

openai_api_key = "YOUR_API_KEY"

"""##### Data Scraping

###### Use BeautifulSoup 4 to scrape the content of a Wikipedia page. Retrieve not just the text but also the headings and sub-headings.
"""

def process_section(section, parent_heading=None):
    subsections_list = []
    subheadings = []
    paragraphs = []
    for subsection in section.find_all(['h3','h2']):
        subsection_heading = subsection.text.strip()
        subsection_paragraphs = subsection.find_next_siblings('p')
        paragraph_texts = []
        for paragraph in subsection_paragraphs:
            paragraph_text = paragraph.text.strip()
            if paragraph_text:
                paragraph_texts.append(paragraph_text)
            else:
                paragraph_texts.append("")  # Replace empty strings with empty string ""
        subheadings.append(subsection_heading)
        paragraphs.append(" ".join(paragraph_texts))
        subsections_list.append(parent_heading)
        # Recursively process nested subsections
        subsections = subsection.find_all(['h3','h2'])
        sub_subsections_list, sub_subheadings, sub_paragraphs = process_section(subsection, parent_heading=subsection_heading)
        subsections_list.extend(sub_subsections_list)
        subheadings.extend(sub_subheadings)
        paragraphs.extend(sub_paragraphs)
    return subsections_list, subheadings, paragraphs

# URL of the Wikipedia page you want to scrape
url = "https://en.wikipedia.org/wiki/Alexander_the_Great"
# Send an HTTP GET request to the URL and parse the HTML content
response = requests.get(url)
soup = BeautifulSoup(response.content, 'html.parser')
# Initialize empty lists to store data
subsections_list, subheadings, paragraphs = process_section(soup)
# Create a DataFrame
data = {
    'section_headings': subsections_list,
    'subsection_headings': subheadings,
    'paragraphs': paragraphs
}
df = pd.DataFrame(data)
# Replace empty strings with "NaN"
df['paragraphs'].replace("", "NaN", inplace=True)

subsection_heading_mains = []
sections = soup.find_all(['h2'])
for section in sections:
    # Extract the section heading
    section_heading = section.text.strip()
    subsection_heading_mains.append(section_heading)
    #print("\nSection Heading:", section_heading)
subsection_heading_mains

pattern = r'\\|\[\d+\]'
# Replace backslashes and numbers with an empty string in the 'Paragraph' column
df['paragraphs'] = df['paragraphs'].str.replace(pattern, '', regex=True)
df['paragraphs'] = df['paragraphs'].str.replace("\\'", "")

df.head(30)

"""##### Once scraped, summarize the text under each section while retaining the original headings.

There are different ways of summarisation of a text:
1. Extractive Text Summarization: The primary goal is to employ a traditional method, initially developed for text summarization, in order to identify and extract the most significant sentences from the original text. It's important to emphasize that the resulting summary will consist of exact sentences directly taken from the original text.
Few ways are:
    1. `gensim.summarization`
    2. `sumy.summarizers.lex_rank`
    3. `sumy.summarizers.lsa`
    4. `sumy.summarizers.luhn`
    5. `sumy.summarizers.kl`


2. Abstractive Text Summarization: The approach involves identifying key sections, understanding context, and succinctly generating sentences in the summary, ensuring core information is conveyed concisely, utilizing the latest advancements in text summarization.This is better than extractive methods where sentences are just selected from original text for the summary. We are going to implement this using `OpenAI Models`
"""

responses = []
texts = []
for i in range(0,1):
    text = df.iloc[i,2]
    messages = [
        {"role":"system", "content":'''summarize the text along with retaining maximum information'''},
        {"role":"user","content":text}
    ]
    #tokens = tokenizer(text, return_tensors='pt')['input_ids']
    #num_tokens = tokens.shape[1]
    temperature = 0.8
    #token_length = num_tokens + 300
    try:
        openai.api_key = "YOUR_API_KEY"
        response = openai.ChatCompletion.create(
            model = 'gpt-3.5-turbo-16k',
            messages = messages,
            temperature = temperature,
            top_p=1,
            frequency_penalty=0,
            presence_penalty=0)
        responses.append(response.choices[0].message.content)
        texts.append(text)
        print(i)
        #time.sleep(10)
    except Exception as e:
        i = i - 1
        print("RateLimitError occurred. Waiting for 20 seconds before continuing...")
        print("Error:", e)
        #time.sleep(30)
        continue

"""The first limitation was that of using gpt-3.5 turbo for summarisation. I was not able to use it because of the input token size limitation

(4,027 tokens).

Therefore lets see the max length of the strings in the dataset
"""

df['paragraphs_len'] = df['paragraphs'].apply(len)
print('Maximum Length of string: ',df['paragraphs_len'].max())
print('Minimum Length of string: ',df['paragraphs_len'].min())

"""On reading on models from openai I found that gpt-3.5-turbo-16k-0613 overcomes this limitation of the dataset by taking input size upon (16,385 tokens)

I have tried various prompts to summarise the information given to maximally. I found the prompt `summarize the text along with retaining maximum information` does a good job.

Other prompts which I tried are:
1. `summarize the text`
2. `summarize the text in less than 100 words`
3. `summarize the text in less than 500 words`
4. `summarize the text in less than 100 words along with retaining maximum information`
5. `summarize the text in less than 500 words along with retaining maximum information`

To address the rate limit error encountered during the summarization process, I ensured that there would be a sufficient gap between the questions posed to the model.
"""

responses = []
texts = []
for i in range(df.shape[0]):
    text = df.iloc[i,2]
    messages = [
        {"role":"system", "content":'''summarize the text along with retaining maximum information'''},
        {"role":"user","content":text}
    ]
    #tokens = tokenizer(text, return_tensors='pt')['input_ids']
    #num_tokens = tokens.shape[1]
    temperature = 0.8
    #token_length = num_tokens + 300
    try:
        openai.api_key = "YOUR_API_KEY"
        response = openai.ChatCompletion.create(
            model = 'gpt-3.5-turbo-16k',
            messages = messages,
            temperature = temperature,
            top_p=1,
            frequency_penalty=0,
            presence_penalty=0)
        responses.append(response.choices[0].message.content)
        texts.append(text)
        print(i)
        time.sleep(20)
    except Exception as e:
        i = i - 1
        print("RateLimitError occurred. Waiting for 20 seconds before continuing...")
        print("Error:", e)
        time.sleep(30)
        continue

len(responses)

df['summaries_normal_prompt'] = responses
df['summaries_normal_prompt_len'] = df['summaries_normal_prompt'].apply(len)
df.head()

"""Generally in reality summaries are there to understand things quickly and its not optimal to be of huge length. Hence now lets restrict it to 80 words."""

responses = []
texts = []
for i in range(df.shape[0]):
    text = df.iloc[i,2]
    messages = [
        {"role":"system", "content":'''summarize the text along with retaining maximum information in 4- 5 sentences, ~80 words.'''},
        {"role":"user","content":text}
    ]
    #tokens = tokenizer(text, return_tensors='pt')['input_ids']
    #num_tokens = tokens.shape[1]
    temperature = 0.8
    #token_length = num_tokens + 300
    try:
        openai.api_key = "YOUR_API_KEY"
        response = openai.ChatCompletion.create(
            model = 'gpt-3.5-turbo-16k',
            messages = messages,
            temperature = temperature,
            top_p=1,
            frequency_penalty=0,
            presence_penalty=0)
        responses.append(response.choices[0].message.content)
        texts.append(text)
        print(i)
        time.sleep(20)
    except Exception as e:
        i = i - 1
        print("RateLimitError occurred. Waiting for 20 seconds before continuing...")
        print("Error:", e)
        time.sleep(30)
        continue
df['summaries_normal_prompt_restrict'] = responses
df['summaries_normal_prompt_restrict_len'] = df['summaries_normal_prompt_restrict'].apply(len)
df.head()

df['summaries_normal_prompt_restrict'] = responses
df['summaries_normal_prompt_restrict_len'] = df['summaries_normal_prompt_restrict'].apply(len)

"""Above we are just giving a prompt and not a prompt chains for generating summaries

##### Create prompts chains for generating summaries that are coherent and contextually accurate.

- One disadvantage observed in long texts is that many models have a limitation of input tokens set to (4027) whereas the length of strings which we extracted is more than 4027 tokens.
- An idea to overcome the above disadvantage is to split the paragraphs into list of strings such that their length stands are less than 4027 tokens and summarise each string. Later we can join the strings.
- The above idea wasnt implemented due to credit limitation by OpenAI but can be a good work of research. (prompt chains + split summarisation)
- A good summary should be detailed and entity-centric without being overly dense and hard to follow.
"""

df.head(2)

prompt = """
In the process of crafting progressively succinct, information-rich summaries of the Wikipedia page dedicated to Alexander the Great, iterate through the following steps five times:

Step 1. Identify 1-3 pertinent Entities (separated by ';') from the Wikipedia page that have not been included in the previously generated summary.
Step 2. Create a new, more compact summary of the exact same length as the previous one, encapsulating not only all entities and details from the preceding summary but also the Missing Entities.

Criteria for a Missing Entity:
Relevance: The entity should directly relate to the life and accomplishments of Alexander the Great.
Specificity: It should be descriptive, coherent, contextually accurate, and concise (limited to 5 words or fewer).
Novelty: The entity must not have been included in the previous summary.
Fidelity: It should be present in the Wikipedia page.
Placement: The entity can be located anywhere within the Wikipedia page.

Additional Guidelines:
-- The initial summary should be a paragraph with minimal specific information beyond the Missing Entities. You can employ verbose language and fillers (e.g., 'the page discusses') to achieve the target word count.
-- Ensure that every word serves a purpose; refine the previous summary for improved flow and to create space for new entities.
-- Utilize techniques like fusion, compression, and the removal of redundant phrases such as 'the page discusses' to maximize conciseness.
-- The summaries should evolve into highly concise, self-contained, coherent, and contextually accurate narratives that can be easily comprehended without the need to reference the Wikipedia page.
-- The Missing Entities can be integrated at any point in the new summary."
- Never drop entities from the previous summary. If space cannot be made, add fewer new entities. Remember to use the exact same number of words for each summary. Answer in JSON. The JSON should be a list (length 5) of dictionaries whose keys are "Missing_Entities" and "Denser_Summary."
"""
responses = []
texts = []
for i in range(df.shape[0]):
    text = df.iloc[i,2]
    messages = [
        {"role":"system", "content":prompt},
        {"role":"user","content":text}
    ]
    #tokens = tokenizer(text, return_tensors='pt')['input_ids']
    #num_tokens = tokens.shape[1]
    temperature = 0.8
    #token_length = num_tokens + 300
    try:
        openai.api_key = "YOUR_API_KEY"
        response = openai.ChatCompletion.create(
            model = 'gpt-3.5-turbo-16k',
            messages = messages,
            temperature = temperature,
            top_p=1,
            frequency_penalty=0,
            presence_penalty=0)
        responses.append(response.choices[0].message.content)
        texts.append(text)
        print(i)
        time.sleep(20)
    except Exception as e:
        i = i - 1
        print("RateLimitError occurred. Waiting for 20 seconds before continuing...")
        print("Error:", e)
        time.sleep(30)
        continue

summaries_from_chain = []

responses[0]

import json

# Your data as a string
data_string = responses[0]

# Parse the data string into a list of dictionaries
data_list = json.loads(data_string)

summary = []
# Retrieve "Denser_Summary" and its key from each dictionary
for entry in data_list:
    denser_summary_key = entry.get("Denser_Summary")
    print("Key:", denser_summary_key)
    summary.append(denser_summary_key)
summaries_from_chain.append(str(summary[:-1]))

summaries_from_chain

file_path = "/content/my_list.txt"

# Open the file for writing
with open(file_path, "w") as file:
    # Iterate through the list and write each item to the file
    for item in responses:
        file.write(item + "\n")

# Optionally, you can close the file explicitly, although using "with" will automatically close it.
# file.close()

print(f"The list has been saved to {file_path}")

df['summaries_chain_prompt_len_len'] = df['summaries_chain_prompt_len'].apply(len)
df['paragraphs_len'] = df['paragraphs'].apply(len)
df['summaries_normal_prompt_len'] = df['summaries_normal_prompt'].apply(len)

# Create a subplot with 1 row and 3 columns (adding the third plot)
plt.figure(figsize=(15, 3))

# Plot the distribution of paragraph lengths
plt.subplot(1, 4, 1)
plt.hist(df['paragraphs_len'], bins=30, color='blue', alpha=0.5, label='Paragraph Length')
plt.xlabel('Length')
plt.ylabel('Frequency')
plt.title('Paragraph Lengths')
plt.xticks(rotation=45)
plt.legend()
plt.grid(True)

# Plot the distribution of summary lengths
plt.subplot(1, 4, 2)
plt.hist(df['summaries_normal_prompt_len'], bins=30, color='red', alpha=0.5, label='Summary Length')
plt.xlabel('Length')
plt.ylabel('Frequency')
plt.title('Normal Prompt')
plt.xticks(rotation=45)
plt.legend()
plt.grid(True)

# Add your third plot here
plt.subplot(1, 4, 3)
plt.hist(df['summaries_normal_prompt_restrict_len'], bins=30, color='green', alpha=0.5, label='Summary Length')
plt.xlabel('Length')
plt.ylabel('Frequency')
plt.title('Prompt with Restriction')
plt.xticks(rotation=45)
plt.legend()
plt.grid(True)

plt.subplot(1, 4, 4)
plt.hist(df['summaries_chain_prompt_len_len'], bins=30, color='orange', alpha=0.5, label='Chain Summary Length')
plt.xlabel('Length')
plt.ylabel('Frequency')
plt.title('Chain Prompting')
plt.xticks(rotation=45)
plt.legend()
plt.grid(True)

plt.suptitle('Dist of Summary Lengths vs Frequency')
# Adjust the layout
plt.tight_layout()

# Show the plots
plt.show()

"""##### Metrics

Original ROUGE paper states that “ROUGE-2 and ROUGE-L worked well in single document summarization tasks” while “ROUGE-1 and ROUGE-L perform great in evaluating short summaries”.
"""

rouge = Rouge()
scores = rouge.get_scores(df.loc[:,'paragraphs'], df.loc[:,'summaries_normal_prompt'])
# Extract precision (p), recall (r), and F1-score (f) values
precision_values = [score['rouge-1']['p'] for score in scores]
recall_values = [score['rouge-1']['r'] for score in scores]
f1_values = [score['rouge-1']['f'] for score in scores]

# Calculate the average
average_precision = np.mean(precision_values)
average_recall = np.mean(recall_values)
average_f1 = np.mean(f1_values)

print(f"Average Precision (p): {average_precision}")
print(f"Average Recall (r): {average_recall}")
print(f"Average F1-Score (f): {average_f1}")

rouge = Rouge()
scores = rouge.get_scores(df.loc[:,'paragraphs'], df.loc[:,'summaries_normal_prompt_restrict'])
# Extract precision (p), recall (r), and F1-score (f) values
precision_values = [score['rouge-1']['p'] for score in scores]
recall_values = [score['rouge-1']['r'] for score in scores]
f1_values = [score['rouge-1']['f'] for score in scores]

# Calculate the average
average_precision = np.mean(precision_values)
average_recall = np.mean(recall_values)
average_f1 = np.mean(f1_values)

print(f"Average Precision (p): {average_precision}")
print(f"Average Recall (r): {average_recall}")
print(f"Average F1-Score (f): {average_f1}")

rouge = Rouge()
scores = rouge.get_scores(df.loc[:,'paragraphs'], df.loc[:,'summaries_chain_prompt_1'])
# Extract precision (p), recall (r), and F1-score (f) values
precision_values = [score['rouge-1']['p'] for score in scores]
recall_values = [score['rouge-1']['r'] for score in scores]
f1_values = [score['rouge-1']['f'] for score in scores]

# Calculate the average
average_precision = np.mean(precision_values)
average_recall = np.mean(recall_values)
average_f1 = np.mean(f1_values)

print(f"Average Precision (p): {average_precision}")
print(f"Average Recall (r): {average_recall}")
print(f"Average F1-Score (f): {average_f1}")

"""This tells that chain prompting is able to do better in machine evaluation."""